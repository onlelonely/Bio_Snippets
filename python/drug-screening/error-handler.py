# ---------------------------------------------
# Source: Drug Screening Pipeline (CPV)
# ---------------------------------------------

"""
éŒ¯èª¤è™•ç†å’Œè‡ªå‹•æ¢å¾©ç³»çµ±
"""

import os
import json
import shutil
import subprocess
from pathlib import Path
import logging
from datetime import datetime
import pandas as pd

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ErrorHandler:
    def __init__(self):
        self.error_dir = Path("logs/errors")
        self.error_dir.mkdir(parents=True, exist_ok=True)
        
        self.recovery_strategies = {
            "docking_failure": self._recover_docking_failure,
            "md_failure": self._recover_md_failure,
            "disk_full": self._handle_disk_full,
            "memory_error": self._handle_memory_error,
            "file_corruption": self._handle_file_corruption
        }
    
    def scan_for_errors(self):
        """æƒæç³»çµ±ä¸­çš„éŒ¯èª¤"""
        logger.info("ğŸ” æƒæç³»çµ±éŒ¯èª¤")
        
        errors_found = []
        
        # 1. æª¢æŸ¥å°æ¥å¤±æ•—
        docking_errors = self._check_docking_errors()
        errors_found.extend(docking_errors)
        
        # 2. æª¢æŸ¥ MD æ¨¡æ“¬å¤±æ•—
        md_errors = self._check_md_errors()
        errors_found.extend(md_errors)
        
        # 3. æª¢æŸ¥ç£ç¢Ÿç©ºé–“
        disk_errors = self._check_disk_space()
        errors_found.extend(disk_errors)
        
        # 4. æª¢æŸ¥æ–‡ä»¶å®Œæ•´æ€§
        file_errors = self._check_file_integrity()
        errors_found.extend(file_errors)
        
        if errors_found:
            logger.warning(f"âš ï¸ ç™¼ç¾ {len(errors_found)} å€‹éŒ¯èª¤")
            for error in errors_found:
                logger.warning(f"   - {error['type']}: {error['description']}")
        else:
            logger.info("âœ… æœªç™¼ç¾ç³»çµ±éŒ¯èª¤")
        
        return errors_found
    
    def _check_docking_errors(self):
        """æª¢æŸ¥å°æ¥éŒ¯èª¤"""
        errors = []
        
        failed_dir = Path("docking_results/failed")
        if failed_dir.exists():
            failed_files = list(failed_dir.glob("*_error.txt"))
            
            for error_file in failed_files:
                try:
                    with open(error_file, 'r') as f:
                        error_content = f.read()
                    
                    # åˆ†æéŒ¯èª¤é¡å‹
                    if "timeout" in error_content.lower():
                        error_type = "docking_timeout"
                    elif "memory" in error_content.lower():
                        error_type = "docking_memory"
                    else:
                        error_type = "docking_failure"
                    
                    errors.append({
                        "type": error_type,
                        "description": f"å°æ¥å¤±æ•—: {error_file.stem}",
                        "file": str(error_file),
                        "details": error_content[:200]
                    })
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ ç„¡æ³•è®€å–éŒ¯èª¤æ–‡ä»¶ {error_file}: {e}")
        
        return errors
    
    def _check_md_errors(self):
        """æª¢æŸ¥ MD æ¨¡æ“¬éŒ¯èª¤"""
        errors = []
        
        md_dir = Path("md_simulations")
        if md_dir.exists():
            for compound_dir in md_dir.iterdir():
                if compound_dir.is_dir():
                    # æª¢æŸ¥æ˜¯å¦æœ‰ .log æ–‡ä»¶ä½†æ²’æœ‰å°æ‡‰çš„è¼¸å‡º
                    log_files = list(compound_dir.glob("*.log"))
                    
                    for log_file in log_files:
                        try:
                            with open(log_file, 'r') as f:
                                log_content = f.read()
                            
                            if "fatal error" in log_content.lower() or "segmentation fault" in log_content.lower():
                                errors.append({
                                    "type": "md_failure",
                                    "description": f"MD æ¨¡æ“¬å¤±æ•—: {compound_dir.name}",
                                    "file": str(log_file),
                                    "details": log_content[-300:]  # æœ€å¾Œ300å­—ç¬¦
                                })
                                
                        except Exception as e:
                            logger.warning(f"âš ï¸ ç„¡æ³•è®€å– MD æ—¥èªŒ {log_file}: {e}")
        
        return errors
    
    def _check_disk_space(self):
        """æª¢æŸ¥ç£ç¢Ÿç©ºé–“"""
        errors = []
        
        import psutil
        disk_usage = psutil.disk_usage('/')
        
        free_percent = (disk_usage.free / disk_usage.total) * 100
        
        if free_percent < 10:
            errors.append({
                "type": "disk_full",
                "description": f"ç£ç¢Ÿç©ºé–“ä¸è¶³: åƒ…å‰© {free_percent:.1f}%",
                "details": f"å¯ç”¨ç©ºé–“: {disk_usage.free / (1024**3):.2f} GB"
            })
        
        return errors
    
    def _check_file_integrity(self):
        """æª¢æŸ¥æ–‡ä»¶å®Œæ•´æ€§"""
        errors = []
        
        # æª¢æŸ¥é‡è¦çš„çµæœæ–‡ä»¶
        important_files = [
            "docking_results/summaries/successful_results.csv",
            "analysis/binding_affinity/binding_affinity_statistics.json",
            "structures/receptors/receptor.pdbqt"
        ]
        
        for file_path in important_files:
            path = Path(file_path)
            
            if path.exists():
                # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦ç‚ºç©º
                if path.stat().st_size == 0:
                    errors.append({
                        "type": "file_corruption",
                        "description": f"æ–‡ä»¶ç‚ºç©º: {file_path}",
                        "file": str(path)
                    })
                
                # æª¢æŸ¥ CSV æ–‡ä»¶æ ¼å¼
                if path.suffix == '.csv':
                    try:
                        df = pd.read_csv(path)
                        if len(df) == 0:
                            errors.append({
                                "type": "file_corruption",
                                "description": f"CSV æ–‡ä»¶ç„¡æ•¸æ“š: {file_path}",
                                "file": str(path)
                            })
                    except Exception:
                        errors.append({
                            "type": "file_corruption",
                            "description": f"CSV æ–‡ä»¶æå£: {file_path}",
                            "file": str(path)
                        })
        
        return errors
    
    def handle_errors(self, errors):
        """è™•ç†ç™¼ç¾çš„éŒ¯èª¤"""
        logger.info("ğŸ”§ é–‹å§‹éŒ¯èª¤è™•ç†")
        
        recovery_results = {}
        
        for error in errors:
            error_type = error["type"]
            
            logger.info(f"   è™•ç†éŒ¯èª¤: {error_type}")
            
            if error_type in self.recovery_strategies:
                try:
                    success = self.recovery_strategies[error_type](error)
                    recovery_results[error["description"]] = success
                    
                    if success:
                        logger.info(f"   âœ… éŒ¯èª¤å·²ä¿®å¾©: {error['description']}")
                    else:
                        logger.warning(f"   âš ï¸ éŒ¯èª¤ä¿®å¾©å¤±æ•—: {error['description']}")
                        
                except Exception as e:
                    logger.error(f"   âŒ éŒ¯èª¤è™•ç†ç•°å¸¸: {e}")
                    recovery_results[error["description"]] = False
            else:
                logger.warning(f"   âš ï¸ ç„¡å¯ç”¨çš„æ¢å¾©ç­–ç•¥: {error_type}")
                recovery_results[error["description"]] = False
        
        # è¨˜éŒ„è™•ç†çµæœ
        self._log_recovery_results(recovery_results)
        
        return recovery_results
    
    def _recover_docking_failure(self, error):
        """æ¢å¾©å°æ¥å¤±æ•—"""
        try:
            # æå–é…é«”åç¨±
            if "file" in error:
                error_file = Path(error["file"])
                ligand_name = error_file.stem.replace("_error", "")
                
                # æª¢æŸ¥åŸå§‹é…é«”æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                ligand_file = Path(f"ligands/processed/high_quality/{ligand_name}.pdbqt")
                
                if ligand_file.exists():
                    # é‡æ–°æäº¤å°æ¥ä»»å‹™
                    logger.info(f"   é‡æ–°æäº¤å°æ¥: {ligand_name}")
                    
                    # é€™è£¡å¯ä»¥é‡æ–°é‹è¡Œå°æ¥
                    # å¯¦éš›å¯¦ç¾éœ€è¦èª¿ç”¨å°æ¥è…³æœ¬
                    return True
                else:
                    logger.warning(f"   æ‰¾ä¸åˆ°é…é«”æ–‡ä»¶: {ligand_file}")
                    return False
            
        except Exception as e:
            logger.error(f"   å°æ¥æ¢å¾©å¤±æ•—: {e}")
        
        return False
    
    def _recover_md_failure(self, error):
        """æ¢å¾© MD æ¨¡æ“¬å¤±æ•—"""
        try:
            if "file" in error:
                log_file = Path(error["file"])
                compound_dir = log_file.parent
                
                # æ¸…ç†å¤±æ•—çš„ MD æ–‡ä»¶
                md_files = ["md.tpr", "md.xtc", "md.edr", "md.log"]
                
                for md_file in md_files:
                    file_path = compound_dir / md_file
                    if file_path.exists():
                        file_path.unlink()
                
                logger.info(f"   æ¸…ç†å¤±æ•—çš„ MD æ–‡ä»¶: {compound_dir}")
                
                # é‡æ–°é–‹å§‹ MD æ¨¡æ“¬
                # å¯¦éš›å¯¦ç¾éœ€è¦èª¿ç”¨ MD è…³æœ¬
                return True
                
        except Exception as e:
            logger.error(f"   MD æ¢å¾©å¤±æ•—: {e}")
        
        return False
    
    def _handle_disk_full(self, error):
        """è™•ç†ç£ç¢Ÿç©ºé–“ä¸è¶³"""
        logger.info("   æ¸…ç†è‡¨æ™‚æ–‡ä»¶")
        
        cleanup_successful = False
        
        try:
            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
            temp_patterns = [
                "**/*.temp.*",
                "**/*~",
                "**/*.bak",
                "**/core.*"
            ]
            
            total_freed = 0
            
            for pattern in temp_patterns:
                temp_files = Path(".").rglob(pattern)
                
                for temp_file in temp_files:
                    if temp_file.is_file():
                        file_size = temp_file.stat().st_size
                        temp_file.unlink()
                        total_freed += file_size
            
            logger.info(f"   æ¸…ç†äº† {total_freed / (1024**2):.2f} MB è‡¨æ™‚æ–‡ä»¶")
            
            # æ¸…ç†èˆŠçš„æ—¥èªŒæ–‡ä»¶ (>7å¤©)
            import time
            current_time = time.time()
            week_ago = current_time - (7 * 24 * 3600)
            
            log_files = Path("logs").rglob("*.log")
            for log_file in log_files:
                if log_file.stat().st_mtime < week_ago:
                    log_file.unlink()
                    logger.info(f"   åˆªé™¤èˆŠæ—¥èªŒ: {log_file}")
            
            cleanup_successful = True
            
        except Exception as e:
            logger.error(f"   ç£ç¢Ÿæ¸…ç†å¤±æ•—: {e}")
        
        return cleanup_successful
    
    def _handle_memory_error(self, error):
        """è™•ç†è¨˜æ†¶é«”éŒ¯èª¤"""
        logger.info("   å˜—è©¦é‡‹æ”¾è¨˜æ†¶é«”")
        
        try:
            # æ®ºæ­»å¯èƒ½çš„æ®­å±é€²ç¨‹
            import psutil
            
            for proc in psutil.process_iter(['pid', 'name', 'status']):
                try:
                    if proc.info['status'] == psutil.STATUS_ZOMBIE:
                        proc.kill()
                        logger.info(f"   çµ‚æ­¢æ®­å±é€²ç¨‹: {proc.info['pid']}")
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
            
            return True
            
        except Exception as e:
            logger.error(f"   è¨˜æ†¶é«”æ¸…ç†å¤±æ•—: {e}")
        
        return False
    
    def _handle_file_corruption(self, error):
        """è™•ç†æ–‡ä»¶æå£"""
        try:
            corrupted_file = Path(error["file"])
            
            # å‰µå»ºå‚™ä»½ç›®éŒ„
            backup_dir = Path("backups")
            backup_dir.mkdir(exist_ok=True)
            
            # å‚™ä»½æå£çš„æ–‡ä»¶
            backup_file = backup_dir / f"{corrupted_file.name}.corrupted.{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            if corrupted_file.exists():
                shutil.move(str(corrupted_file), str(backup_file))
                logger.info(f"   å·²å‚™ä»½æå£æ–‡ä»¶: {backup_file}")
            
            # å˜—è©¦å¾å…¶ä»–ä¾†æºæ¢å¾©æ–‡ä»¶
            # é€™è£¡å¯ä»¥å¯¦ç¾å…·é«”çš„æ–‡ä»¶æ¢å¾©é‚è¼¯
            
            return True
            
        except Exception as e:
            logger.error(f"   æ–‡ä»¶æ¢å¾©å¤±æ•—: {e}")
        
        return False
    
    def _log_recovery_results(self, results):
        """è¨˜éŒ„æ¢å¾©çµæœ"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "recovery_results": results,
            "total_errors": len(results),
            "successful_recoveries": sum(1 for success in results.values() if success)
        }
        
        log_file = self.error_dir / "recovery_log.jsonl"
        
        with open(log_file, 'a') as f:
            f.write(json.dumps(log_entry) + '\n')
    
    def run_error_handling_cycle(self):
        """é‹è¡Œå®Œæ•´çš„éŒ¯èª¤è™•ç†å¾ªç’°"""
        logger.info("ğŸš€ é–‹å§‹éŒ¯èª¤è™•ç†å¾ªç’°")
        
        try:
            # æƒæéŒ¯èª¤
            errors = self.scan_for_errors()
            
            if errors:
                # è™•ç†éŒ¯èª¤
                recovery_results = self.handle_errors(errors)
                
                successful_recoveries = sum(1 for success in recovery_results.values() if success)
                
                logger.info(f"ğŸ“Š éŒ¯èª¤è™•ç†å®Œæˆ:")
                logger.info(f"   ç¸½éŒ¯èª¤æ•¸: {len(errors)}")
                logger.info(f"   æˆåŠŸæ¢å¾©: {successful_recoveries}")
                logger.info(f"   æ¢å¾©ç‡: {successful_recoveries/len(errors)*100:.1f}%")
                
                return recovery_results
            else:
                logger.info("âœ… æœªç™¼ç¾éœ€è¦è™•ç†çš„éŒ¯èª¤")
                return {}
                
        except Exception as e:
            logger.error(f"âŒ éŒ¯èª¤è™•ç†å¾ªç’°å¤±æ•—: {e}")
            return {}

if __name__ == "__main__":
    handler = ErrorHandler()
    handler.run_error_handling_cycle()