# ---------------------------------------------
# Source: Drug Screening Pipeline (CPV)
# ---------------------------------------------

"""
å¤§è¦æ¨¡å¹³è¡Œåˆ†å­å°æ¥åŸ·è¡Œ
"""

import os
import subprocess
import multiprocessing
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
import logging
import time
import json
import shutil

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ParallelDocker:
    def __init__(self, 
                 ligand_dir="ligands/processed/high_quality",
                 receptor_file="structures/receptors/receptor.pdbqt",
                 config_file="docking_results/vina_config.txt",
                 output_dir="docking_results/individual"):
        
        self.ligand_dir = Path(ligand_dir)
        self.receptor_file = Path(receptor_file)
        self.config_file = Path(config_file)
        self.output_dir = Path(output_dir)
        self.failed_dir = Path("docking_results/failed")
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.failed_dir.mkdir(parents=True, exist_ok=True)
        
        # çµ±è¨ˆä¿¡æ¯
        self.stats = {
            "total_ligands": 0,
            "successful_dockings": 0,
            "failed_dockings": 0,
            "start_time": None,
            "end_time": None
        }
    
    def find_ligand_files(self):
        """æŸ¥æ‰¾æ‰€æœ‰é…é«”æ–‡ä»¶"""
        ligand_files = list(self.ligand_dir.glob("*.pdbqt"))
        
        if not ligand_files:
            logger.error(f"âŒ åœ¨ {self.ligand_dir} ä¸­æœªæ‰¾åˆ° PDBQT æ–‡ä»¶")
            return []
        
        logger.info(f"ğŸ“ æ‰¾åˆ° {len(ligand_files)} å€‹é…é«”æ–‡ä»¶")
        self.stats["total_ligands"] = len(ligand_files)
        
        return ligand_files
    
    def dock_single_ligand(self, ligand_file):
        """å°æ¥å–®å€‹é…é«”"""
        ligand_name = ligand_file.stem
        output_file = self.output_dir / f"{ligand_name}_result.pdbqt"
        log_file = self.output_dir / f"{ligand_name}_log.txt"
        
        # å¦‚æœçµæœå·²å­˜åœ¨ï¼Œè·³é
        if output_file.exists() and log_file.exists():
            return self._parse_existing_result(log_file, ligand_name)
        
        cmd = [
            "vina",
            "--receptor", str(self.receptor_file),
            "--ligand", str(ligand_file),
            "--config", str(self.config_file),
            "--out", str(output_file),
            "--log", str(log_file)
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            
            if result.returncode == 0 and output_file.exists():
                return self._parse_docking_result(log_file, ligand_name, "success")
            else:
                # ç§»å‹•å¤±æ•—çš„æ–‡ä»¶
                self._handle_failed_docking(ligand_file, ligand_name, result.stderr)
                return self._parse_docking_result(None, ligand_name, "failed", result.stderr)
                
        except subprocess.TimeoutExpired:
            error_msg = "Docking timeout"
            self._handle_failed_docking(ligand_file, ligand_name, error_msg)
            return self._parse_docking_result(None, ligand_name, "failed", error_msg)
        except Exception as e:
            error_msg = str(e)
            self._handle_failed_docking(ligand_file, ligand_name, error_msg)
            return self._parse_docking_result(None, ligand_name, "failed", error_msg)
    
    def _parse_existing_result(self, log_file, ligand_name):
        """è§£æå·²å­˜åœ¨çš„çµæœ"""
        result = self._parse_docking_result(log_file, ligand_name, "success")
        if result["binding_affinity"] is not None:
            return result
        else:
            # å¦‚æœç„¡æ³•è§£æï¼Œè¦–ç‚ºå¤±æ•—
            return self._parse_docking_result(None, ligand_name, "failed", "Could not parse existing result")
    
    def _parse_docking_result(self, log_file, ligand_name, status, error_msg=None):
        """è§£æå°æ¥çµæœ"""
        result = {
            "ligand_name": ligand_name,
            "status": status,
            "binding_affinity": None,
            "error_message": error_msg
        }
        
        if status == "success" and log_file and Path(log_file).exists():
            try:
                with open(log_file, 'r') as f:
                    content = f.read()
                
                # æå–æœ€ä½³çµåˆèƒ½
                import re
                scores = re.findall(r'\s+1\s+([-\d.]+)', content)
                
                if scores:
                    result["binding_affinity"] = float(scores[0])
                else:
                    result["status"] = "failed"
                    result["error_message"] = "Could not parse binding affinity"
                    
            except Exception as e:
                result["status"] = "failed"
                result["error_message"] = f"Log parsing error: {e}"
        
        # æ›´æ–°çµ±è¨ˆ
        if result["status"] == "success":
            self.stats["successful_dockings"] += 1
        else:
            self.stats["failed_dockings"] += 1
        
        return result
    
    def _handle_failed_docking(self, ligand_file, ligand_name, error_msg):
        """è™•ç†å¤±æ•—çš„å°æ¥"""
        # è¤‡è£½å¤±æ•—çš„é…é«”æ–‡ä»¶åˆ°å¤±æ•—ç›®éŒ„
        failed_ligand = self.failed_dir / ligand_file.name
        try:
            shutil.copy2(ligand_file, failed_ligand)
        except Exception:
            pass
        
        # è¨˜éŒ„éŒ¯èª¤
        error_log = self.failed_dir / f"{ligand_name}_error.txt"
        with open(error_log, 'w') as f:
            f.write(f"Ligand: {ligand_name}\n")
            f.write(f"Error: {error_msg}\n")
            f.write(f"Time: {time.ctime()}\n")
    
    def create_batches(self, ligand_files, batch_size=1000):
        """å°‡é…é«”æ–‡ä»¶åˆ†æ‰¹è™•ç†"""
        batches = []
        for i in range(0, len(ligand_files), batch_size):
            batch = ligand_files[i:i+batch_size]
            batches.append(batch)
        
        logger.info(f"ğŸ“¦ å°‡ {len(ligand_files)} å€‹é…é«”åˆ†ç‚º {len(batches)} æ‰¹")
        return batches
    
    def run_parallel_docking(self, max_workers=None, batch_size=1000):
        """åŸ·è¡Œå¹³è¡Œå°æ¥"""
        if max_workers is None:
            max_workers = min(multiprocessing.cpu_count(), 8)
        
        logger.info(f"ğŸš€ é–‹å§‹å¹³è¡Œå°æ¥ï¼Œä½¿ç”¨ {max_workers} å€‹é€²ç¨‹")
        
        # æŸ¥æ‰¾é…é«”æ–‡ä»¶
        ligand_files = self.find_ligand_files()
        if not ligand_files:
            return False
        
        # åˆ†æ‰¹è™•ç†
        batches = self.create_batches(ligand_files, batch_size)
        
        all_results = []
        self.stats["start_time"] = time.time()
        
        try:
            with ProcessPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»å‹™
                future_to_ligand = {
                    executor.submit(self.dock_single_ligand, ligand_file): ligand_file 
                    for ligand_file in ligand_files
                }
                
                # æ”¶é›†çµæœ
                completed = 0
                for future in as_completed(future_to_ligand):
                    ligand_file = future_to_ligand[future]
                    
                    try:
                        result = future.result()
                        all_results.append(result)
                        completed += 1
                        
                        # æ¯ 100 å€‹å®Œæˆæ™‚å ±å‘Šé€²åº¦
                        if completed % 100 == 0:
                            logger.info(f"ğŸ“ˆ é€²åº¦: {completed}/{len(ligand_files)} "
                                      f"({completed/len(ligand_files)*100:.1f}%)")
                            
                    except Exception as e:
                        logger.error(f"âŒ è™•ç† {ligand_file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        except KeyboardInterrupt:
            logger.warning("âš ï¸ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨ä¿å­˜ç•¶å‰çµæœ...")
        
        self.stats["end_time"] = time.time()
        self.stats["duration"] = self.stats["end_time"] - self.stats["start_time"]
        
        # ä¿å­˜çµæœ
        self.save_results(all_results)
        
        return len(all_results) > 0
    
    def save_results(self, results):
        """ä¿å­˜å°æ¥çµæœ"""
        logger.info("ğŸ’¾ ä¿å­˜å°æ¥çµæœ")
        
        # ä¿å­˜è©³ç´°çµæœ
        import pandas as pd
        df = pd.DataFrame(results)
        
        results_file = Path("docking_results/summaries/all_docking_results.csv")
        results_file.parent.mkdir(parents=True, exist_ok=True)
        df.to_csv(results_file, index=False)
        
        # ä¿å­˜æˆåŠŸçš„çµæœï¼ˆæ’åºï¼‰
        successful_results = df[df["status"] == "success"].copy()
        if not successful_results.empty:
            successful_results = successful_results.sort_values("binding_affinity")
            
            success_file = Path("docking_results/summaries/successful_results.csv")
            successful_results.to_csv(success_file, index=False)
            
            # ä¿å­˜å‰ 1000 å
            top_results = successful_results.head(1000)
            top_file = Path("docking_results/summaries/top_1000_results.csv")
            top_results.to_csv(top_file, index=False)
        
        # ä¿å­˜çµ±è¨ˆä¿¡æ¯
        final_stats = {
            **self.stats,
            "success_rate": self.stats["successful_dockings"] / self.stats["total_ligands"] if self.stats["total_ligands"] > 0 else 0,
            "dockings_per_second": self.stats["successful_dockings"] / self.stats["duration"] if self.stats["duration"] > 0 else 0
        }
        
        stats_file = Path("docking_results/summaries/docking_statistics.json")
        with open(stats_file, 'w') as f:
            json.dump(final_stats, f, indent=2)
        
        # æ‰“å°çµ±è¨ˆä¿¡æ¯
        logger.info("ğŸ“Š å°æ¥çµ±è¨ˆ:")
        logger.info(f"   ç¸½é…é«”æ•¸: {final_stats['total_ligands']}")
        logger.info(f"   æˆåŠŸå°æ¥: {final_stats['successful_dockings']}")
        logger.info(f"   å¤±æ•—å°æ¥: {final_stats['failed_dockings']}")
        logger.info(f"   æˆåŠŸç‡: {final_stats['success_rate']:.1%}")
        logger.info(f"   ç¸½è€—æ™‚: {final_stats['duration']/3600:.1f} å°æ™‚")
        logger.info(f"   é€Ÿåº¦: {final_stats['dockings_per_second']:.1f} å°æ¥/ç§’")
        
        logger.info(f"ğŸ“ çµæœæ–‡ä»¶:")
        logger.info(f"   ğŸ“Š {results_file}")
        if not successful_results.empty:
            logger.info(f"   âœ… {success_file}")
            logger.info(f"   ğŸ† {top_file}")
        logger.info(f"   ğŸ“ˆ {stats_file}")
    
    def resume_docking(self):
        """æ¢å¾©ä¸­æ–·çš„å°æ¥"""
        logger.info("ğŸ”„ æ¢å¾©å°æ¥ä»»å‹™")
        
        # æª¢æŸ¥å·²å®Œæˆçš„å°æ¥
        completed_files = set()
        if self.output_dir.exists():
            for log_file in self.output_dir.glob("*_log.txt"):
                ligand_name = log_file.stem.replace("_log", "")
                completed_files.add(f"{ligand_name}.pdbqt")
        
        logger.info(f"ğŸ“‹ å·²å®Œæˆ {len(completed_files)} å€‹å°æ¥")
        
        # æ‰¾åˆ°å‰©é¤˜çš„é…é«”
        all_ligands = self.find_ligand_files()
        remaining_ligands = [lig for lig in all_ligands 
                           if lig.name not in completed_files]
        
        if not remaining_ligands:
            logger.info("âœ… æ‰€æœ‰å°æ¥å·²å®Œæˆ")
            return True
        
        logger.info(f"ğŸ”„ é‚„éœ€å°æ¥ {len(remaining_ligands)} å€‹é…é«”")
        
        # ç¹¼çºŒå°æ¥å‰©é¤˜çš„é…é«”
        return self.run_parallel_docking()

if __name__ == "__main__":
    Docker = ParallelDocker()
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºæ¢å¾©æ¨¡å¼
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--resume":
        success = docker.resume_docking()
    else:
        success = docker.run_parallel_docking()
    
    if success:
        print("âœ… å°æ¥å®Œæˆ")
    else:
        print("âŒ å°æ¥å¤±æ•—")