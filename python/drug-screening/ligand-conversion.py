# ---------------------------------------------
# Source: Drug Screening Pipeline (CPV)
# ---------------------------------------------

"""
æ‰¹æ¬¡è½‰æ›é…é«”åº«ç‚º AutoDock Vina æ ¼å¼
"""

import os
import subprocess
import multiprocessing
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
import logging
import time
from rdkit import Chem
from rdkit.Chem import rdMolDescriptors
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LigandConverter:
    def __init__(self, input_dir="ligands/raw", output_dir="ligands/processed"):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.stats = {
            "total_input": 0,
            "successful_conversions": 0,
            "failed_conversions": 0,
            "start_time": None,
            "end_time": None
        }
    
    def split_sdf_library(self, sdf_file, chunk_size=1000):
        """å°‡å¤§å‹ SDF æ–‡ä»¶åˆ†å‰²ç‚ºå°å¡Šä¾¿æ–¼ä¸¦è¡Œè™•ç†"""
        logger.info(f"ğŸ“¦ åˆ†å‰² SDF æ–‡ä»¶: {sdf_file}")
        
        supplier = Chem.SDMolSupplier(str(sdf_file), removeHs=False)
        chunk_files = []
        current_chunk = []
        chunk_num = 0
        
        for i, mol in enumerate(supplier):
            if mol is None:
                continue
                
            current_chunk.append(mol)
            
            if len(current_chunk) >= chunk_size:
                # ä¿å­˜ç•¶å‰å¡Š
                chunk_file = self.output_dir / f"chunk_{chunk_num:04d}.sdf"
                writer = Chem.SDWriter(str(chunk_file))
                
                for chunk_mol in current_chunk:
                    writer.write(chunk_mol)
                writer.close()
                
                chunk_files.append(chunk_file)
                chunk_num += 1
                current_chunk = []
                
                logger.info(f"   å‰µå»ºå¡Š {chunk_num}: {chunk_file}")
        
        # è™•ç†æœ€å¾Œä¸€å¡Š
        if current_chunk:
            chunk_file = self.output_dir / f"chunk_{chunk_num:04d}.sdf"
            writer = Chem.SDWriter(str(chunk_file))
            
            for chunk_mol in current_chunk:
                writer.write(chunk_mol)
            writer.close()
            
            chunk_files.append(chunk_file)
        
        logger.info(f"âœ… åˆ†å‰²å®Œæˆï¼Œå…±å‰µå»º {len(chunk_files)} å€‹å¡Š")
        return chunk_files
    
    def convert_single_molecule(self, mol_data):
        """è½‰æ›å–®å€‹åˆ†å­"""
        mol, mol_id, output_path = mol_data
        
        try:
            # åŸºæœ¬åˆ†å­é©—è­‰
            if mol is None:
                return False, f"Invalid molecule {mol_id}"
            
            # æª¢æŸ¥åˆ†å­æ˜¯å¦ç¬¦åˆåŸºæœ¬æ¨™æº–
            mw = rdMolDescriptors.CalcExactMolWt(mol)
            if mw < 100 or mw > 600:
                return False, f"Molecular weight out of range: {mw}"
            
            # ä½¿ç”¨ RDKit ç”Ÿæˆ PDB æ ¼å¼
            mol_with_h = Chem.AddHs(mol)
            
            # å˜—è©¦ç”Ÿæˆ 3D åæ¨™
            from rdkit.Chem import AllChem
            if AllChem.EmbedMolecule(mol_with_h) == 0:
                AllChem.MMFFOptimizeMolecule(mol_with_h)
            
            # ä¿å­˜ç‚ºè‡¨æ™‚ PDB æ–‡ä»¶
            temp_pdb = output_path.with_suffix('.pdb')
            Chem.MolToPDBFile(mol_with_h, str(temp_pdb))
            
            # ä½¿ç”¨ OpenBabel è½‰æ›ç‚º PDBQT
            cmd = [
                "obabel",
                str(temp_pdb),
                "-O", str(output_path),
                "-p", "7.4",  # pH 7.4
                "--gen3d"
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
            if temp_pdb.exists():
                temp_pdb.unlink()
            
            if result.returncode == 0 and output_path.exists():
                return True, f"Successfully converted {mol_id}"
            else:
                return False, f"OpenBabel failed for {mol_id}: {result.stderr}"
                
        except Exception as e:
            return False, f"Exception converting {mol_id}: {str(e)}"
    
    def process_sdf_chunk(self, chunk_file):
        """è™•ç†å–®å€‹ SDF å¡Š"""
        logger.info(f"ğŸ”„ è™•ç†å¡Š: {chunk_file}")
        
        supplier = Chem.SDMolSupplier(str(chunk_file), removeHs=False)
        results = []
        
        for i, mol in enumerate(supplier):
            if mol is None:
                continue
            
            # ç”Ÿæˆè¼¸å‡ºæ–‡ä»¶å
            mol_id = f"{chunk_file.stem}_{i:04d}"
            output_file = self.output_dir / f"{mol_id}.pdbqt"
            
            # è½‰æ›åˆ†å­
            success, message = self.convert_single_molecule((mol, mol_id, output_file))
            results.append((success, message))
            
            if success:
                self.stats["successful_conversions"] += 1
            else:
                self.stats["failed_conversions"] += 1
        
        # æ¸…ç†å¡Šæ–‡ä»¶
        chunk_file.unlink()
        
        return results
    
    def parallel_conversion(self, sdf_files, max_workers=None):
        """ä¸¦è¡Œè½‰æ›é…é«”æ–‡ä»¶"""
        if max_workers is None:
            max_workers = min(multiprocessing.cpu_count(), 8)
        
        logger.info(f"ğŸš€ é–‹å§‹ä¸¦è¡Œè½‰æ›ï¼Œä½¿ç”¨ {max_workers} å€‹é€²ç¨‹")
        
        # é¦–å…ˆåˆ†å‰²æ‰€æœ‰ SDF æ–‡ä»¶
        all_chunks = []
        for sdf_file in sdf_files:
            chunks = self.split_sdf_library(sdf_file)
            all_chunks.extend(chunks)
        
        self.stats["total_input"] = len(all_chunks) * 1000  # ä¼°ç®—
        
        # ä¸¦è¡Œè™•ç†æ‰€æœ‰å¡Š
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(self.process_sdf_chunk, chunk): chunk 
                      for chunk in all_chunks}
            
            for i, future in enumerate(as_completed(futures)):
                chunk = futures[future]
                try:
                    results = future.result()
                    logger.info(f"âœ… å®Œæˆå¡Š {i+1}/{len(all_chunks)}: {chunk}")
                except Exception as e:
                    logger.error(f"âŒ è™•ç†å¡Šå¤±æ•— {chunk}: {e}")
    
    def validate_conversions(self):
        """é©—è­‰è½‰æ›çµæœ"""
        logger.info("ğŸ” é©—è­‰è½‰æ›çµæœ...")
        
        pdbqt_files = list(self.output_dir.glob("*.pdbqt"))
        valid_files = []
        
        for pdbqt_file in pdbqt_files:
            try:
                # æª¢æŸ¥æ–‡ä»¶å¤§å°
                if pdbqt_file.stat().st_size < 100:
                    logger.warning(f"âš ï¸ æ–‡ä»¶éå°: {pdbqt_file}")
                    continue
                
                # æª¢æŸ¥ PDBQT æ ¼å¼
                with open(pdbqt_file, 'r') as f:
                    content = f.read()
                    
                # åŸºæœ¬æ ¼å¼æª¢æŸ¥
                if "ATOM" in content and "TORSDOF" in content:
                    valid_files.append(pdbqt_file)
                else:
                    logger.warning(f"âš ï¸ æ ¼å¼ç„¡æ•ˆ: {pdbqt_file}")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ æª¢æŸ¥æ–‡ä»¶å¤±æ•— {pdbqt_file}: {e}")
        
        validation_stats = {
            "total_pdbqt_files": len(pdbqt_files),
            "valid_files": len(valid_files),
            "invalid_files": len(pdbqt_files) - len(valid_files),
            "validation_rate": len(valid_files) / len(pdbqt_files) if pdbqt_files else 0
        }
        
        logger.info(f"ğŸ“Š é©—è­‰çµ±è¨ˆ:")
        logger.info(f"   ç¸½æ–‡ä»¶æ•¸: {validation_stats['total_pdbqt_files']}")
        logger.info(f"   æœ‰æ•ˆæ–‡ä»¶: {validation_stats['valid_files']}")
        logger.info(f"   ç„¡æ•ˆæ–‡ä»¶: {validation_stats['invalid_files']}")
        logger.info(f"   æœ‰æ•ˆç‡: {validation_stats['validation_rate']:.2%}")
        
        return validation_stats
    
    def run_conversion(self):
        """åŸ·è¡Œå®Œæ•´çš„è½‰æ›æµç¨‹"""
        logger.info("ğŸš€ é–‹å§‹é…é«”åº«è½‰æ›")
        self.stats["start_time"] = time.time()
        
        try:
            # æŸ¥æ‰¾è¼¸å…¥ SDF æ–‡ä»¶
            sdf_files = list(self.input_dir.glob("*.sdf"))
            
            if not sdf_files:
                logger.error("âŒ æœªæ‰¾åˆ° SDF æ–‡ä»¶")
                return False
            
            logger.info(f"ğŸ“ æ‰¾åˆ° {len(sdf_files)} å€‹ SDF æ–‡ä»¶")
            
            # åŸ·è¡Œä¸¦è¡Œè½‰æ›
            self.parallel_conversion(sdf_files)
            
            # é©—è­‰çµæœ
            validation_stats = self.validate_conversions()
            
            self.stats["end_time"] = time.time()
            self.stats["duration"] = self.stats["end_time"] - self.stats["start_time"]
            
            # ä¿å­˜çµ±è¨ˆä¿¡æ¯
            final_stats = {**self.stats, **validation_stats}
            
            stats_file = self.output_dir / "conversion_stats.json"
            with open(stats_file, 'w') as f:
                json.dump(final_stats, f, indent=2)
            
            logger.info("ğŸ‰ é…é«”è½‰æ›å®Œæˆ!")
            logger.info(f"ğŸ“Š æœ€çµ‚çµ±è¨ˆ: {final_stats}")
            logger.info(f"ğŸ’¾ çµ±è¨ˆä¿¡æ¯ä¿å­˜è‡³: {stats_file}")
            
            return validation_stats["validation_rate"] > 0.8
            
        except Exception as e:
            logger.error(f"âŒ è½‰æ›éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

if __name__ == "__main__":
    converter = LigandConverter()
    success = converter.run_conversion()
    
    if success:
        print("âœ… é…é«”åº«è½‰æ›æˆåŠŸ")
    else:
        print("âŒ é…é«”åº«è½‰æ›å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ—¥èªŒ")